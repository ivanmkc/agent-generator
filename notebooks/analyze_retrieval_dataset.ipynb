{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Dataset Analysis Report\n",
    "\n",
    "## 1. Methodology: Causal Relevance Verification\n",
    "\n",
    "This report analyzes the empirical relevance of documentation for solving software engineering tasks. Instead of relying on static similarity (embeddings) or human labeling, we use **Monte Carlo Causal Inference**.\n",
    "\n",
    "### The Algorithm\n",
    "For each query $q$ and a pool of candidate documents $C(q)$: \n",
    "1.  **Randomized Trials:** We execute $N$ trials. In each trial, a random subset $S \\subset C(q)$ is injected into the LLM's context window.\n",
    "2.  **Blind Solving:** The LLM attempts to solve the task using *only* $S$.\n",
    "3.  **Impact Scoring (Delta P):** We calculate the marginal contribution of each document $d$: \n",
    "    $$ \\Delta P(d) = P(\text{Success} | d \\in S) - P(\text{Success} | d \notin S) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "# Configure plotting\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Latest Run Logs (YAML Stream)\n",
    "trial_events = []\n",
    "convergence_events = []\n",
    "trials_df = pd.DataFrame()\n",
    "\n",
    "log_files = glob.glob(\"../logs/*.yaml\")\n",
    "if not log_files:\n",
    "    # Try relative to root if running from there\n",
    "    log_files = glob.glob(\"logs/*.yaml\")\n",
    "\n",
    "if not log_files:\n",
    "    print(\"No log files found.\")\n",
    "else:\n",
    "    latest_log = max(log_files, key=os.path.getctime)\n",
    "    print(f\"Loading logs from: {latest_log}\")\n",
    "    \n",
    "    all_events = []\n",
    "    with open(latest_log, 'r') as f:\n",
    "        events = list(yaml.safe_load_all(f))\n",
    "        all_events = events\n",
    "    \n",
    "    # Extract Trial Events\n",
    "    trial_events = [e for e in all_events if e and e.get('event') == 'trial_complete']\n",
    "    trials_df = pd.DataFrame(trial_events)\n",
    "    \n",
    "    # Extract Convergence Events\n",
    "    convergence_events = [e for e in all_events if e and e.get('event') == 'convergence_check']\n",
    "    \n",
    "    print(f\"Loaded {len(trials_df)} trials and {len(convergence_events)} convergence checks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Impact Score Distribution (Signal vs. Noise)\n",
    "\n",
    "This Kernel Density Estimate (KDE) plot visualizes the distribution of utility across different candidate sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset Metadata\n",
    "dataset_path = Path(\"../retrieval_dataset_verified.yaml\")\n",
    "if not dataset_path.exists():\n",
    "    dataset_path = Path(\"retrieval_dataset_verified.yaml\")\n",
    "\n",
    "if dataset_path.exists():\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    \n",
    "    cases = data.get('cases', [])\n",
    "    records = []\n",
    "    convergence_traces = []\n",
    "    for case in cases:\n",
    "        if 'convergence_trace' in case.get('metadata', {}):\n",
    "            convergence_traces.append({\n",
    "                'case_id': case['id'],\n",
    "                'trace': case['metadata']['convergence_trace']\n",
    "            })\n",
    "        candidates = case.get('candidates', [])\n",
    "        for ctx in candidates:\n",
    "            meta = ctx.get('metadata', { })\n",
    "            records.append({\n",
    "                'case_id': case['id'],\n",
    "                'fqn': ctx['fqn'],\n",
    "                'source_type': ctx['type'],\n",
    "                'delta_p': meta.get('delta_p', 0.0),\n",
    "            })\n",
    "    stats_df = pd.DataFrame(records)\n",
    "\n",
    "    if not stats_df.empty:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.kdeplot(data=stats_df, x='delta_p', hue='source_type', fill=True, common_norm=False)\n",
    "        plt.title('Impact Score Density by Candidate Source')\n",
    "        plt.xlabel('Delta P (Impact Score)')\n",
    "        plt.axvline(0, color='red', linestyle='--')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Dataset file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Case-Level Convergence (Max Uncertainty)\n",
    "\n",
    "This plot shows the **Maximum Standard Error** across all candidates for each case. This metric determines when we stop testing a case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'convergence_traces' in locals() and convergence_traces:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for trace in convergence_traces:\n",
    "        y = trace['trace']\n",
    "        x = range(1, len(y) + 1)\n",
    "        plt.plot(x, y, alpha=0.5, label=trace['case_id'][:20])\n",
    "\n",
    "    plt.title('Convergence of Case Uncertainty (Max SE) over Trials')\n",
    "    plt.xlabel('Trial Number')\n",
    "    plt.ylabel('Max Standard Error')\n",
    "    plt.axhline(0.1, color='red', linestyle='--', label='Threshold (0.1)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Dive: Per-Context Convergence\n",
    "\n",
    "This visualization drills down into a single case, showing the uncertainty (Standard Error) for **each individual document** over time. \n",
    "\n",
    "**Interpretation:**\n",
    "*   **Fan-out:** Lines start high (SE=1.0) and drop.\n",
    "*   **Separation:** Some docs converge quickly (easy to judge), others stay high (noisy/ambiguous).\n",
    "*   **The Governor:** The slowest-converging line (highest Y) is what holds up the entire case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'convergence_events' in locals() and convergence_events:\n",
    "    # Select the first case found in logs\n",
    "    example_case_id = convergence_events[0]['case_id']\n",
    "    print(f\"Analyzing specific convergence for case: {example_case_id}\")\n",
    "    \n",
    "    # Extract SE history for this case\n",
    "    se_history = []\n",
    "    for e in convergence_events:\n",
    "        if e['case_id'] == example_case_id and 'se_map' in e:\n",
    "            for fqn, se in e['se_map'].items():\n",
    "                se_history.append({\n",
    "                    'trial': e['trial_index'],\n",
    "                    'fqn': fqn,\n",
    "                    'standard_error': se\n",
    "                })\n",
    "    \n",
    "    se_df = pd.DataFrame(se_history)\n",
    "    \n",
    "    if not se_df.empty:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.lineplot(data=se_df, x='trial', y='standard_error', hue='fqn', legend=False, alpha=0.6, linewidth=1)\n",
    "        \n",
    "        # Highlight the max envelope\n",
    "        max_se_per_trial = se_df.groupby('trial')['standard_error'].max()\n",
    "        plt.plot(max_se_per_trial.index, max_se_per_trial.values, color='black', linestyle='--', linewidth=2, label='Max Envelope')\n",
    "        \n",
    "        plt.title(f'Per-Context Convergence Trajectories for {example_case_id[:30]}...')\n",
    "        plt.ylabel('Standard Error (Uncertainty)')\n",
    "        plt.xlabel('Trial Number')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trial Dynamics (Success Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'trials_df' in locals() and not trials_df.empty:\n",
    "    trials_df['success'] = trials_df['is_correct'].astype(int)\n",
    "    trials_df['cumulative_success_rate'] = trials_df.groupby('case_id')['success'].expanding().mean().reset_index(level=0, drop=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=trials_df, x='trial_index', y='cumulative_success_rate', hue='case_id', alpha=0.7)\n",
    "    plt.title('Cumulative Success Rate by Case')\n",
    "    plt.xlabel('Trial Index')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}