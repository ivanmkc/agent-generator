{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Dataset Analysis Report\n\n",
    "## 1. Methodology: Causal Relevance Verification\n\n",
    "This report analyzes the empirical relevance of documentation for solving software engineering tasks. Instead of relying on static similarity (embeddings) or human labeling, we use **Monte Carlo Causal Inference**. \n\n",
    "### The Algorithm\n",
    "For each query $q$ and a pool of candidate documents $C(q)$: \n",
    "1.  **Randomized Trials:** We execute $N$ trials. In each trial, a random subset $S \\subset C(q)$ is injected into the LLM's context window.\n",
    "2.  **Blind Solving:** The LLM attempts to solve the task using *only* $S$.\n",
    "3.  **Impact Scoring (Delta P):** We calculate the marginal contribution of each document $d$: \n",
    "    $$ \\Delta P(d) = P(\text{Success} | d \\in S) - P(\text{Success} | d \notin S) $$ \n",
    "### Interpretation\n",
    "*   **$\\Delta P \u0007pprox 1.0$**: **Critical**. The task cannot be solved without this document.\n",
    "*   **$\\Delta P > 0.1$**: **Helpful**. The document improves success rate but might be redundant with others.\n",
    "*   **$\\Delta P \u0007pprox 0.0$**: **Irrelevant**. The document is noise.\n",
    "*   **$\\Delta P < -0.1$**: **Toxic**. The document actively misleads the model (e.g., hallucination trigger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# Add root to path to import lib\n",
    "sys.path.append('..')\n",
    "from tools.retrieval_dataset_generation.lib import RetrievalDataset, RetrievalCase\n",
    "\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "# Configure plotting\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Latest Run Logs (YAML Stream)\n",
    "trial_events = []\n",
    "convergence_events = []\n",
    "trials_df = pd.DataFrame()\n",
    "\n",
    "log_files = glob.glob(\"../logs/*.yaml\")\n",
    "if not log_files:\n",
    "    # Try relative to root if running from there\n",
    "    log_files = glob.glob(\"logs/*.yaml\")\n",
    "\n",
    "if not log_files:\n",
    "    print(\"No log files found.\")\n",
    "else:\n",
    "    latest_log = max(log_files, key=os.path.getctime)\n",
    "    print(f\"Loading logs from: {latest_log}\")\n",
    "    \n",
    "    all_events = []\n",
    "    with open(latest_log, 'r') as f:\n",
    "        events = list(yaml.safe_load_all(f))\n",
    "        all_events = events\n",
    "    \n",
    "    # Extract Trial Events\n",
    "    trial_events = [e for e in all_events if e and e.get('event') == 'trial_complete']\n",
    "    trials_df = pd.DataFrame(trial_events)\n",
    "    \n",
    "    # Extract Convergence Events\n",
    "    convergence_events = [e for e in all_events if e and e.get('event') == 'convergence_check']\n",
    "    \n",
    "    print(f\"Loaded {len(trials_df)} trials and {len(convergence_events)} convergence checks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Impact Score Distribution (Signal vs. Noise)\n",
    "\n",
    "This Kernel Density Estimate (KDE) plot visualizes the distribution of utility across different candidate sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset Metadata\n",
    "dataset_path = Path(\"../retrieval_dataset_verified.yaml\")\n",
    "if not dataset_path.exists():\n",
    "    dataset_path = Path(\"retrieval_dataset_verified.yaml\")\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    raise FileNotFoundError(\"Dataset file not found.\")\n",
    "\n",
    "print(f\"Loading metadata from: {dataset_path}\")\n",
    "with open(dataset_path, 'r') as f:\n",
    "    dataset = RetrievalDataset.model_validate(yaml.safe_load(f))\n",
    "\n",
    "print(f\"Loaded {len(dataset.cases)} cases.\")\n",
    "\n",
    "cases = dataset.cases\n",
    "records = []\n",
    "convergence_traces = []\n",
    "for case in cases:\n",
    "    # Use Pydantic dot notation or dict access on metadata dict\n",
    "    if 'convergence_trace' in case.metadata:\n",
    "        convergence_traces.append({\n",
    "            'case_id': case.id,\n",
    "            'trace': case.metadata['convergence_trace']\n",
    "        })\n",
    "    candidates = case.candidates\n",
    "    for ctx in candidates:\n",
    "        meta = ctx.metadata\n",
    "        records.append({\n",
    "            'case_id': case.id,\n",
    "            'fqn': ctx.fqn,\n",
    "            'source_type': ctx.context_type,\n",
    "            'delta_p': meta.delta_p,\n",
    "            'n_in': meta.n_in,\n",
    "        })\n",
    "stats_df = pd.DataFrame(records)\n",
    "\n",
    "if not stats_df.empty:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(data=stats_df, x='delta_p', hue='source_type', fill=True, common_norm=False)\n",
    "    plt.title('Impact Score Density by Candidate Source')\n",
    "    plt.xlabel('Delta P (Impact Score)')\n",
    "    plt.axvline(0, color='red', linestyle='--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Case-Level Convergence (Max Uncertainty)\n",
    "\n",
    "This plot shows the **Maximum Standard Error** across all candidates for each case. This metric determines when we stop testing a case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for trace in convergence_traces:\n",
    "    y = trace['trace']\n",
    "    x = range(1, len(y) + 1)\n",
    "    plt.plot(x, y, alpha=0.5, label=trace['case_id'][:20])\n",
    "\n",
    "    plt.title('Convergence of Case Uncertainty (Max SE) over Trials')\n",
    "    plt.xlabel('Trial Number')\n",
    "    plt.ylabel('Max Standard Error')\n",
    "    plt.axhline(0.1, color='red', linestyle='--', label='Threshold (0.1)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Dive: Per-Context Convergence\n",
    "\n",
    "This visualization drills down into a single case, showing the uncertainty (Standard Error) for **each individual document** over time. \n",
    "\n",
    "**Interpretation:**\n",
    "*   **Fan-out:** Lines start high (SE=1.0) and drop.\n",
    "*   **Separation:** Some docs converge quickly (easy to judge), others stay high (noisy/ambiguous).\n",
    "*   **The Governor:** The slowest-converging line (highest Y) is what holds up the entire case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first case found in logs\n",
    "if convergence_events:\n",
    "    example_case_id = convergence_events[0]['case_id']\n",
    "    print(f\"Analyzing specific convergence for case: {example_case_id}\")\n",
    "    \n",
    "    # Extract SE history for this case\n",
    "    se_history = []\n",
    "    for e in convergence_events:\n",
    "        if e['case_id'] == example_case_id:\n",
    "            # Strict check: se_map is guaranteed by Pydantic model\n",
    "            if 'se_map' in e:\n",
    "                for fqn, se in e['se_map'].items():\n",
    "                    se_history.append({\n",
    "                        'trial': e['trial_index'],\n",
    "                        'fqn': fqn,\n",
    "                        'standard_error': se\n",
    "                    })\n",
    "    \n",
    "    se_df = pd.DataFrame(se_history)\n",
    "    \n",
    "    if not se_df.empty:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.lineplot(data=se_df, x='trial', y='standard_error', hue='fqn', legend=False, alpha=0.6, linewidth=1)\n",
    "        \n",
    "        # Highlight the max envelope\n",
    "        max_se_per_trial = se_df.groupby('trial')['standard_error'].max()\n",
    "        plt.plot(max_se_per_trial.index, max_se_per_trial.values, color='black', linestyle='--', linewidth=2, label='Max Envelope')\n",
    "        \n",
    "        plt.title(f'Per-Context Convergence Trajectories for {example_case_id[:30]}...')\n",
    "        plt.ylabel('Standard Error (Uncertainty)')\n",
    "        plt.xlabel('Trial Number')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trial Dynamics (Success Rate)\n",
    "\n",
    "This graph tracks the stability of the model's success rate over the course of the Monte Carlo trials.\n",
    "\n",
    "*   **Rising Curve:** The model success rate is stable or improving as we sample more contexts.\n",
    "*   **High Variance (Jagged):** The model is **brittle**. Its success depends heavily on exactly which subset of documents is present in the context. This indicates that the \"Winning Coalition\" is rare or fragile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not trials_df.empty:\n",
    "    trials_df['success'] = trials_df['is_correct'].astype(int)\n",
    "    trials_df['cumulative_success_rate'] = trials_df.groupby('case_id')['success'].expanding().mean().reset_index(level=0, drop=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=trials_df, x='trial_index', y='cumulative_success_rate', hue='case_id', alpha=0.7)\n",
    "    plt.title('Cumulative Success Rate by Case')\n",
    "    plt.xlabel('Trial Index')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Case Breakdown: Relevance Analysis\n",
    "\n",
    "This section provides a detailed breakdown of candidate documents for **every case**, sorted by their causal relevance (Delta P).\n",
    "\n",
    "**Legend:**\n",
    "*   **$\\Delta P > 0.1$:** **Primary Signal**. These documents are likely essential.\n",
    "*   **Source Type:** Where this candidate came from (Gold Mining, Vector Search, or Random Noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "if not stats_df.empty:\n",
    "    # Group by Case ID\n",
    "    cases_groups = stats_df.groupby('case_id')\n",
    "    \n",
    "    for case_id, group in cases_groups:\n",
    "        print()\n",
    "        print(f\"# Case: {case_id}\")\n",
    "        \n",
    "        # Get query text from dataset object (not in stats_df)\n",
    "        case_obj = next((c for c in dataset.cases if c.id == case_id), None)\n",
    "        if case_obj:\n",
    "             print(f\"**Query:** {case_obj.query}\")\n",
    "             print(f\"**Zero-Context Success:** {case_obj.metadata.get('zero_context_success_rate', 'N/A')}\")\n",
    "        \n",
    "        # Sort by Delta P descending\n",
    "        sorted_group = group.sort_values('delta_p', ascending=False)\n",
    "        \n",
    "        # Categorize\n",
    "        relevant = sorted_group[sorted_group['delta_p'] > 0.1]\n",
    "        neutral = sorted_group[(sorted_group['delta_p'] <= 0.1) & (sorted_group['delta_p'] >= -0.1)]\n",
    "        toxic = sorted_group[sorted_group['delta_p'] < -0.1]\n",
    "        \n",
    "        display_cols = ['fqn', 'delta_p', 'source_type', 'n_in']\n",
    "        \n",
    "        if not relevant.empty:\n",
    "            print()\n",
    "            print(\"### \u2705 Relevant Documents (Delta P > 0.1)\")\n",
    "            print(relevant[display_cols].to_markdown(index=False))\n",
    "        else:\n",
    "            print()\n",
    "            print(\"*No highly relevant documents found.*\")\n",
    "            \n",
    "        if not toxic.empty:\n",
    "             print()\n",
    "             print(\"### \u274c Toxic Documents (Delta P < -0.1)\")\n",
    "             print(toxic[display_cols].to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}