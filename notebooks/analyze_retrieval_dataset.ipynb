{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Dataset Analysis\n",
    "\n",
    "This notebook analyzes the `retrieval_dataset_verified.yaml` generated by the validation pipeline. It visualizes the empirical relevance of contexts mined from benchmarks vs. vector search.\n",
    "\n",
    "## Methodology: Monte Carlo Relevance Verification\n",
    "\n",
    "The dataset was generated using a rigorous statistical approach to determine which documentation is actually useful for solving a task.\n",
    "\n",
    "1.  **Candidate Pooling:** For each benchmark query, we gathered candidates from:\n",
    "    *   **Gold Mining:** Extracting class names from the benchmark answer or solution code.\n",
    "    *   **Vector Search:** Retrieving top-k results using `text-embedding-004`.\n",
    "    *   **Random Noise:** Adding random documents as a control group.\n",
    "\n",
    "2.  **Monte Carlo Trials:** We ran `N=3` Bernoulli trials (p=0.5) per case. In each trial, a random subset of candidates was injected into the prompt of a `gemini-2.5-flash` agent.\n",
    "\n",
    "3.  **Causal Impact Scoring (Delta P):** We measured the impact of each document on the agent's success rate:\n",
    "    $$\Delta P = P(\text{Success} | \text{Context Present}) - P(\text{Success} | \text{Context Absent})$$",
    "\n",
    "    *   **Positive $\Delta P$:** The document helps.\n",
    "    *   **Zero/Negative $\Delta P$:** The document is irrelevant or harmful (distractor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dataset_path = Path(\"../retrieval_dataset_verified.yaml\")\n",
    "if not dataset_path.exists():\n",
    "    print(f\"File not found: {dataset_path}\")\n",
    "else:\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    \n",
    "    cases = data.get('cases', [])\n",
    "    print(f\"Loaded {len(cases)} cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Candidates into a DataFrame\n",
    "records = []\n",
    "for case in cases:\n",
    "    # Access lists safely\n",
    "    pos = case.get('positive_ctxs', []) or []\n",
    "    neg = case.get('negative_ctxs', []) or []\n",
    "    \n",
    "    candidates = pos + neg\n",
    "    for ctx in candidates:\n",
    "        meta = ctx.get('metadata', { })\n",
    "        records.append({\n",
    "            'case_id': case['id'],\n",
    "            'query': case['query'],\n",
    "            'fqn': ctx['fqn'],\n",
    "            'source_type': ctx['type'], # gold, retrieved, negative\n",
    "            'empirical_relevance': ctx.get('empirical_relevance', 'UNKNOWN'),\n",
    "            'delta_p': meta.get('delta_p', 0.0),\n",
    "            'p_in': meta.get('p_in', 0.0),\n",
    "            'p_out': meta.get('p_out', 0.0)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"Total Candidates Analyzed: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Relevance Distribution by Source\n",
    "\n",
    "This chart shows the efficiency of our candidate generation sources:\n",
    "*   **gold**: Candidates derived from ground truth metadata.\n",
    "*   **retrieved**: Candidates found by Vector Search.\n",
    "*   **negative**: Randomly sampled noise.\n",
    "\n",
    "**Interpretation:** We expect `gold` to have high relevance. If `retrieved` has high relevance, it means Vector Search is successfully finding useful documents that might not have been explicitly listed in the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_counts = df.groupby(['source_type', 'empirical_relevance']).size().unstack(fill_value=0)\n",
    "print(relevance_counts)\n",
    "\n",
    "relevance_counts.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "plt.title('Empirical Relevance by Source Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Impact Score (Delta P) Distribution\n",
    "\n",
    "This histogram shows the distribution of causal impact scores.\n",
    "\n",
    "*   **Right Tail (> 0.05):** The \"Signal\". These documents actively help the model.\n",
    "*   **Center (~ 0.0):** The \"Noise\". These documents don't matter.\n",
    "*   **Left Tail (< 0.0):** The \"Toxins\". These documents confuse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='delta_p', hue='empirical_relevance', bins=20, multiple=\"stack\")\n",
    "plt.title('Distribution of Impact Scores (Delta P)')\n",
    "plt.xlabel('Delta P (P_in - P_out)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. High Impact Contexts\n",
    "\n",
    "These are the \"VIP\" documents that provided the biggest boost to performance. Analyzing these can reveal what kind of information the model is missing internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_impact = df[df['empirical_relevance'] == 'YES'].sort_values('delta_p', ascending=False).head(20)\n",
    "top_impact[['fqn', 'delta_p', 'source_type', 'case_id']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}