{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b024cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script to run benchmarks and analyze results.\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add root to sys.path if not there\n",
    "if str(Path.cwd()) not in sys.path:\n",
    "    sys.path.append(str(Path.cwd()))\n",
    "\n",
    "import difflib\n",
    "import pandas as pd\n",
    "\n",
    "from benchmarks import benchmark_orchestrator\n",
    "from benchmarks.benchmark_candidates import CANDIDATE_GENERATORS\n",
    "from benchmarks.config import MAX_BENCHMARK_CONCURRENCY\n",
    "from benchmarks.data_models import BenchmarkRunResult\n",
    "from benchmarks.logger import JsonTraceLogger\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes for colors\n",
    "class Bcolors:\n",
    "  HEADER = \"\\033[95m\"\n",
    "  OKBLUE = \"\\033[94m\"\n",
    "  OKCYAN = \"\\033[96m\"\n",
    "  OKGREEN = \"\\033[92m\"\n",
    "  WARNING = \"\\033[93m\"\n",
    "  FAIL = \"\\033[91m\"\n",
    "  ENDC = \"\\033[0m\"\n",
    "  BOLD = \"\\033[1m\"\n",
    "  UNDERLINE = \"\\033[4m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4494716",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_comparison(logger: JsonTraceLogger) -> List[BenchmarkRunResult]:\n",
    "  \"\"\"Sets up and runs the benchmark comparison.\"\"\"\n",
    "  print(\"Configuring benchmark run...\")\n",
    "\n",
    "  benchmark_suites = [\n",
    "      \"benchmarks/benchmark_definitions/api_understanding/benchmark.yaml\",\n",
    "      \"benchmarks/benchmark_definitions/fix_errors/benchmark.yaml\",\n",
    "      \"benchmarks/benchmark_definitions/diagnose_setup_errors_mc/benchmark.yaml\",\n",
    "      \"benchmarks/benchmark_definitions/configure_adk_features_mc/benchmark.yaml\",\n",
    "      \"benchmarks/benchmark_definitions/predict_runtime_behavior_mc/benchmark.yaml\",\n",
    "  ]\n",
    "\n",
    "  answer_generators = CANDIDATE_GENERATORS\n",
    "\n",
    "  print(f\"Executing benchmarks with {len(answer_generators)} generators...\")\n",
    "  # max_concurrency matches Cloud Run instance memory limits (defined in config.py).\n",
    "  # High concurrency causes OOM due to multiple Node.js CLI processes (~200MB each).\n",
    "  # Reduced to 10 for local Podman stability.\n",
    "  benchmark_results = await benchmark_orchestrator.run_benchmarks(\n",
    "      benchmark_suites=benchmark_suites,\n",
    "      answer_generators=answer_generators,\n",
    "      max_concurrency=MAX_BENCHMARK_CONCURRENCY,\n",
    "      max_retries=3,\n",
    "      logger=logger,\n",
    "  )\n",
    "\n",
    "  return benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_error_type(row) -> str:\n",
    "  \"\"\"Extracts error type from the result row.\"\"\"\n",
    "  if \"error_type\" in row and pd.notna(row[\"error_type\"]):\n",
    "    # If it's an Enum object (from pydantic validation), get its value\n",
    "    et = row[\"error_type\"]\n",
    "    if hasattr(et, \"value\"):\n",
    "      return et.value\n",
    "    return str(et)\n",
    "  return \"OtherError\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98dad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(\n",
    "    benchmark_run_results: List[BenchmarkRunResult],\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"Converts results to a DataFrame and adds derived columns.\"\"\"\n",
    "  raw_results_df = pd.DataFrame([r.model_dump() for r in benchmark_run_results])\n",
    "\n",
    "  if not raw_results_df.empty:\n",
    "    raw_results_df[\"suite\"] = raw_results_df[\"suite\"].apply(\n",
    "        lambda x: x.split(\"/\")[-2]\n",
    "    )\n",
    "    raw_results_df[\"final_error_type\"] = raw_results_df.apply(\n",
    "        extract_error_type, axis=1\n",
    "    )\n",
    "    # Ensure status is a string\n",
    "    raw_results_df[\"status_str\"] = raw_results_df[\"status\"].apply(\n",
    "        lambda x: x.value if hasattr(x, \"value\") else str(x)\n",
    "    )\n",
    "  return raw_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(raw_results_df: pd.DataFrame):\n",
    "  \"\"\"Prints a high-level summary of pass rates.\"\"\"\n",
    "  if raw_results_df.empty:\n",
    "    print(\"No results to summarize.\")\n",
    "    return\n",
    "\n",
    "  # Calculate counts\n",
    "  # passed: result == 1\n",
    "  # crashes: status_str == 'fail_crash'\n",
    "  # valid_failures: status_str == 'fail_validation'\n",
    "  \n",
    "  df = raw_results_df.copy()\n",
    "  df[\"is_crash\"] = df[\"status_str\"] == \"fail_crash\"\n",
    "  \n",
    "  # 1. General Pass/Total Summary\n",
    "  summary_df = df.groupby([\"answer_generator\", \"suite\"]).agg(\n",
    "      passed=(\"result\", \"sum\"),\n",
    "      crashes=(\"is_crash\", \"sum\"),\n",
    "      total=(\"result\", \"count\"),\n",
    "  )\n",
    "  \n",
    "  # Overall System Pass Rate: Passed / Total\n",
    "  summary_df[\"system_pass_rate\"] = summary_df[\"passed\"] / summary_df[\"total\"]\n",
    "  \n",
    "  # Model Accuracy: Passed / (Total - Crashes)\n",
    "  # Represents quality on valid attempts (excluding infra errors/rate limits)\n",
    "  summary_df[\"valid_attempts\"] = summary_df[\"total\"] - summary_df[\"crashes\"]\n",
    "  summary_df[\"model_accuracy\"] = summary_df.apply(\n",
    "      lambda row: row[\"passed\"] / row[\"valid_attempts\"] if row[\"valid_attempts\"] > 0 else 0.0,\n",
    "      axis=1\n",
    "  )\n",
    "\n",
    "  print(f\"{Bcolors.HEADER}--- Benchmark Summary ---\\n{Bcolors.ENDC}\")\n",
    "  # Format as percentages\n",
    "  display_df = summary_df.copy()\n",
    "  display_df[\"system_pass_rate\"] = display_df[\"system_pass_rate\"].map(\"{:.1%}\".format)\n",
    "  display_df[\"model_accuracy\"] = display_df[\"model_accuracy\"].map(\"{:.1%}\".format)\n",
    "  print(display_df[[\"passed\", \"crashes\", \"total\", \"system_pass_rate\", \"model_accuracy\"]])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8383fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(raw_results_df: pd.DataFrame):\n",
    "  \"\"\"Prints performance and cost metrics.\"\"\"\n",
    "  if raw_results_df.empty:\n",
    "    return\n",
    "\n",
    "  # Extract usage metrics if available\n",
    "  df = raw_results_df.copy()\n",
    "\n",
    "  def get_meta(row, key):\n",
    "    if isinstance(row.get(\"usage_metadata\"), dict):\n",
    "      return row[\"usage_metadata\"].get(key, 0)\n",
    "    return 0\n",
    "\n",
    "  df[\"tokens\"] = df.apply(lambda r: get_meta(r, \"total_tokens\"), axis=1)\n",
    "  df[\"cost\"] = df.apply(lambda r: get_meta(r, \"cost\"), axis=1)\n",
    "  df[\"is_crash\"] = df[\"status_str\"] == \"fail_crash\"\n",
    "\n",
    "  # Helper to aggregate metrics including the new rates\n",
    "  def aggregate_metrics(grouped):\n",
    "    return grouped.agg(\n",
    "        avg_latency=(\"latency\", \"mean\"),\n",
    "        avg_tokens=(\"tokens\", \"mean\"),\n",
    "        total_cost=(\"cost\", \"sum\"),\n",
    "        passed=(\"result\", \"sum\"),\n",
    "        crashes=(\"is_crash\", \"sum\"),\n",
    "        count=(\"result\", \"count\"),\n",
    "    )\n",
    "\n",
    "  def calculate_rates(metrics_df):\n",
    "    metrics_df[\"system_pass_rate\"] = metrics_df[\"passed\"] / metrics_df[\"count\"]\n",
    "    \n",
    "    valid_attempts = metrics_df[\"count\"] - metrics_df[\"crashes\"]\n",
    "    metrics_df[\"model_accuracy\"] = metrics_df.apply(\n",
    "        lambda row: row[\"passed\"] / valid_attempts[row.name] if valid_attempts[row.name] > 0 else 0.0,\n",
    "        axis=1\n",
    "    )\n",
    "    return metrics_df.drop(columns=[\"passed\", \"crashes\"]) # Keep count, maybe remove others to keep table clean\n",
    "\n",
    "  # 1. Per Answer Generator Breakdown\n",
    "  gen_metrics = aggregate_metrics(df.groupby(\"answer_generator\"))\n",
    "  gen_metrics = calculate_rates(gen_metrics)\n",
    "  gen_metrics = gen_metrics.rename(columns={\"avg_latency\": \"avg_latency (s)\"})\n",
    "\n",
    "  print(f\"{Bcolors.HEADER}--- Metrics by Answer Generator ---\\n{Bcolors.ENDC}\")\n",
    "  print(gen_metrics.round(4))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  # 2. Detailed Breakdown by Generator and Suite\n",
    "  detailed_metrics = aggregate_metrics(df.groupby([\"answer_generator\", \"suite\"]))\n",
    "  detailed_metrics = calculate_rates(detailed_metrics)\n",
    "  detailed_metrics = detailed_metrics.rename(\n",
    "      columns={\"avg_latency\": \"avg_latency (s)\"}\n",
    "  )\n",
    "\n",
    "  print(f\"{Bcolors.HEADER}--- Metrics by Generator & Suite ---\\n{Bcolors.ENDC}\")\n",
    "  print(detailed_metrics.round(4))\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c4a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time_profiling(raw_results_df: pd.DataFrame):\n",
    "  \"\"\"Analyzes latency and execution time to identify bottlenecks.\"\"\"\n",
    "  if raw_results_df.empty:\n",
    "    return\n",
    "\n",
    "  print(f\"{Bcolors.HEADER}--- Time Profiling Analysis ---\\n{Bcolors.ENDC}\")\n",
    "\n",
    "  # 1. Latency Statistics\n",
    "  latency_stats = raw_results_df.groupby(\"answer_generator\")[\n",
    "      \"latency\"\n",
    "  ].describe(percentiles=[0.5, 0.75, 0.90, 0.95])[\n",
    "      [\"mean\", \"min\", \"50%\", \"90%\", \"95%\", \"max\"]\n",
    "  ]\n",
    "  print(\"Latency Statistics (seconds):\")\n",
    "  print(latency_stats.round(2))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  # 2. Slowest Benchmarks\n",
    "  print(\"Top 5 Slowest Benchmarks:\")\n",
    "  slowest = raw_results_df.nlargest(5, \"latency\")[\n",
    "      [\"answer_generator\", \"suite\", \"benchmark_name\", \"latency\", \"result\"]\n",
    "  ]\n",
    "  print(slowest.to_string(index=False))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  # 3. Complexity Analysis (Trace Logs)\n",
    "  # Count model calls and tool calls if trace logs exist\n",
    "  def count_trace_events(logs):\n",
    "    if not isinstance(logs, list):\n",
    "      return 0, 0\n",
    "    model_calls = sum(\n",
    "        1 for e in logs if e.get(\"type\") in [\"model_response\", \"model_call\"]\n",
    "    )\n",
    "    tool_calls = sum(\n",
    "        1 for e in logs if e.get(\"type\") in [\"tool_code\", \"tool_execution\"]\n",
    "    )\n",
    "    return model_calls, tool_calls\n",
    "\n",
    "  # Check if trace_logs column exists and has data\n",
    "  if \"trace_logs\" in raw_results_df.columns:\n",
    "    # Use apply with result_type='expand' to get two columns\n",
    "    counts = raw_results_df[\"trace_logs\"].apply(\n",
    "        lambda x: count_trace_events(x if isinstance(x, list) else [])\n",
    "    )\n",
    "    # Manually assign columns to avoid inconsistencies\n",
    "    raw_results_df[\"num_model_calls\"] = counts.apply(lambda x: x[0])\n",
    "    raw_results_df[\"num_tool_calls\"] = counts.apply(lambda x: x[1])\n",
    "\n",
    "    # Average calls per generator\n",
    "    call_stats = raw_results_df.groupby(\"answer_generator\")[\n",
    "        [\"num_model_calls\", \"num_tool_calls\", \"latency\"]\n",
    "    ].mean()\n",
    "    print(\"Average Complexity (Calls) vs Latency:\")\n",
    "    print(call_stats.round(2))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf9dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_detailed_breakdown(raw_results_df: pd.DataFrame):\n",
    "  \"\"\"Prints detailed error breakdown and Gemini CLI failures.\"\"\"\n",
    "  if raw_results_df.empty:\n",
    "    return\n",
    "\n",
    "  # Filter for failures only\n",
    "  failed_df = raw_results_df[raw_results_df[\"result\"] == 0]\n",
    "\n",
    "  if not failed_df.empty:\n",
    "    # Calculate counts per error type\n",
    "    error_counts = (\n",
    "        failed_df.groupby([\"answer_generator\", \"suite\", \"final_error_type\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "\n",
    "    # Merge with total counts to calculate ratios relative to total runs\n",
    "    # First, get total counts per generator/suite group\n",
    "    total_counts = (\n",
    "        raw_results_df.groupby([\"answer_generator\", \"suite\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"total_runs\")\n",
    "    )\n",
    "\n",
    "    # Merge error counts with totals\n",
    "    error_summary = pd.merge(\n",
    "        error_counts, total_counts, on=[\"answer_generator\", \"suite\"]\n",
    "    )\n",
    "\n",
    "    # Calculate failure rate for each specific error type\n",
    "    error_summary[\"failure_ratio\"] = (\n",
    "        error_summary[\"count\"] / error_summary[\"total_runs\"]\n",
    "    )\n",
    "\n",
    "    print(f\"{Bcolors.HEADER}--- Detailed Error Breakdown ---\\n{Bcolors.ENDC}\")\n",
    "    # Sort for better readability\n",
    "    error_summary = error_summary.sort_values(\n",
    "        [\"answer_generator\", \"suite\", \"count\"], ascending=[True, True, False]\n",
    "    )\n",
    "    print(error_summary.to_string(index=False))\n",
    "\n",
    "    # --- DETAILED DEBUG FOR GEMINI CLI FAILURES ---\n",
    "    print(\n",
    "        f\"\\n{Bcolors.FAIL}--- DETAILED GEMINI CLI FAILURES ---\\n{Bcolors.ENDC}\"\n",
    "    )\n",
    "    cli_failures = failed_df[\n",
    "        failed_df[\"answer_generator\"].str.contains(\"GeminiCliAnswerGenerator\")\n",
    "    ]\n",
    "    if not cli_failures.empty:\n",
    "      # Print just the first 3 failures to avoid overwhelming output\n",
    "      for _, failure_row in cli_failures.head(3).iterrows():\n",
    "        print(\n",
    "            f\"\\nBenchmark: {failure_row['benchmark_name']} (Suite:\"\n",
    "            f\" {failure_row['suite']})\"\n",
    "        )\n",
    "        print(f\"Error Type: {failure_row['final_error_type']}\")\n",
    "        print(f\"Full Validation Error:\\n{failure_row['validation_error']}\")\n",
    "        print(\"-\" * 60)\n",
    "    else:\n",
    "      print(\"No Gemini CLI failures found in this run.\")\n",
    "    # -----------------------------------------------\n",
    "  else:\n",
    "    print(f\"{Bcolors.OKGREEN}No failures detected!{Bcolors.ENDC}\")\n",
    "\n",
    "def strip_ansi(text: str) -> str:\n",
    "  \"\"\"Strips ANSI escape codes from the text.\"\"\"\n",
    "  ansi_escape = re.compile(r\"\\x1B(?:[@-Z\\-_]|[0-?]*[ -/]*[@-~])\")\n",
    "  return ansi_escape.sub(\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ab633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_detailed_reports(raw_results_df: pd.DataFrame, output_dir: Path):\n",
    "  \"\"\"Generates detailed Markdown reports for each answer generator.\"\"\"\n",
    "  if raw_results_df.empty:\n",
    "    return\n",
    "\n",
    "  print(f\"\\n{Bcolors.HEADER}--- Generating Detailed Reports ---{Bcolors.ENDC}\")\n",
    "  print(f\"Output Directory: {output_dir.absolute()}\")\n",
    "\n",
    "  for generator, group in raw_results_df.groupby(\"answer_generator\"):\n",
    "    report_lines = [f\"# Benchmark Report: {generator}\", \"\"]\n",
    "\n",
    "    # Summary\n",
    "    total = len(group)\n",
    "    passed = len(group[group[\"result\"] == 1])\n",
    "    pass_rate = (passed / total) * 100 if total > 0 else 0\n",
    "\n",
    "    report_lines.extend([\n",
    "        \"## Summary\",\n",
    "        f\"- **Total Cases:** {total}\",\n",
    "        f\"- **Passed:** {passed}\",\n",
    "        f\"- **Pass Rate:** {pass_rate:.2f}%\",\n",
    "        \"\",\n",
    "        \"## Details\",\n",
    "        \"\",\n",
    "    ])\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "      status_icon = \"✅\" if row[\"result\"] == 1 else \"❌\"\n",
    "      report_lines.extend([\n",
    "          f\"### {status_icon} {row['benchmark_name']}\",\n",
    "          f\"- **Suite:** {row['suite']}\",\n",
    "          f\"- **Status:** {row['status']}\",\n",
    "          f\"- **Error Type:** {row['final_error_type']}\",\n",
    "          \"\",\n",
    "      ])\n",
    "\n",
    "      if row[\"validation_error\"]:\n",
    "        cleaned_error = strip_ansi(str(row[\"validation_error\"]))\n",
    "        report_lines.extend(\n",
    "            [\"**Validation Error:**\", \"```\", cleaned_error, \"```\", \"\"]\n",
    "        )\n",
    "\n",
    "      if row[\"rationale\"]:\n",
    "        cleaned_rationale = strip_ansi(str(row[\"rationale\"]))\n",
    "        report_lines.extend([\"**Rationale:**\", cleaned_rationale, \"\"])\n",
    "\n",
    "        report_lines.extend([\n",
    "            \"**Generated Answer:**\",\n",
    "            \"```python\",\n",
    "            str(row[\"answer\"]),\n",
    "            \"```\",\n",
    "        ])\n",
    "\n",
    "        # Add diff for fix_errors benchmarks\n",
    "        if row[\"benchmark_type\"] == \"fix_error\":\n",
    "          fixed_content = row.get(\"ground_truth\", \"\") or \"\"\n",
    "\n",
    "          generated_content = str(row[\"answer\"])\n",
    "          diff = difflib.unified_diff(\n",
    "              [l.rstrip() for l in fixed_content.splitlines(keepends=True)],\n",
    "              [l.rstrip() for l in generated_content.splitlines(keepends=True)],\n",
    "              fromfile=\"expected/fixed.py\",\n",
    "              tofile=\"generated/answer.py\",\n",
    "          )\n",
    "\n",
    "          diff_text = \"\\n\".join(diff)\n",
    "\n",
    "          if diff_text:\n",
    "            report_lines.extend([\n",
    "                \"**Diff (Expected vs. Generated):**\",\n",
    "                \"```diff\",\n",
    "                diff_text,\n",
    "                \"```\",\n",
    "            ])\n",
    "          else:\n",
    "            report_lines.extend([\n",
    "                \"**Diff (Expected vs. Generated):**\",\n",
    "                \"```\",\n",
    "                \"No differences.\",\n",
    "                \"```\",\n",
    "            ])\n",
    "\n",
    "        elif row[\"benchmark_type\"] == \"api_understanding\":\n",
    "          expected_content = row.get(\"ground_truth\", \"\") or \"\"\n",
    "          report_lines.extend([\n",
    "              \"**Expected Answer (Example):**\",\n",
    "              \"```python\",\n",
    "              expected_content,\n",
    "              \"```\",\n",
    "          ])\n",
    "\n",
    "        elif row[\"benchmark_type\"] == \"multiple_choice\":\n",
    "          expected_content = row.get(\"ground_truth\", \"\") or \"\"\n",
    "          report_lines.extend([\n",
    "              f\"**Expected Answer:** {expected_content}\",\n",
    "              \"\",\n",
    "          ])\n",
    "\n",
    "        report_lines.extend([\"---\", \"\"])\n",
    "\n",
    "    # Sanitize filename\n",
    "    safe_name = \"\".join(\n",
    "        c if c.isalnum() or c in \"._- \" else \"_\" for c in generator\n",
    "    )\n",
    "    file_path = output_dir / f\"{safe_name}_report.md\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "      f.write(\"\\n\".join(report_lines))\n",
    "      print(f\"  - Report saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58ee04",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell for papermill\n",
    "run_output_dir_str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup unified output directory\n",
    "if run_output_dir_str:\n",
    "  run_output_dir = Path(run_output_dir_str)\n",
    "else:\n",
    "  current_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "  run_output_dir = Path(\"benchmark_runs\") / current_timestamp\n",
    "\n",
    "run_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize logger\n",
    "logger = JsonTraceLogger(output_dir=str(run_output_dir), filename=\"trace.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2483ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the benchmarks\n",
    "benchmark_run_results = await run_comparison(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_results_df = process_results(benchmark_run_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ddb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary(raw_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(raw_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e025752",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_time_profiling(raw_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_detailed_breakdown(raw_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_detailed_reports(raw_results_df, output_dir=run_output_dir)\n",
    "\n",
    "logger.finalize_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
