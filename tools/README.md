# ADK Benchmark Tools

This directory contains utility scripts for managing and debugging ADK benchmark runs.

## Debugging Tool (`debugging.py`)

This script helps you analyze the `results.json` and `trace.jsonl` files generated by a benchmark run. It is useful for investigating why an agent failed (e.g., did it use tools? did the tools return errors?).

### Usage

1.  **List Failed Cases:**
    To see which benchmarks failed in a specific run:
    ```bash
    python tools/debugging.py --run-dir benchmark_runs/<timestamp> --list-cases
    ```

2.  **Analyze a Specific Case:**
    To see tool calls, inputs, and outputs for a specific failed case:
    ```bash
    python tools/debugging.py --run-dir benchmark_runs/<timestamp> --case "01: A minimal LlmAgent"
    ```

### Example Output

```text
Analyzing Case: 01: A minimal LlmAgent.
Status: fail_validation
Error Type: Signature Verification Failed

--- Attempt 1 ---
Tool Calls:
  - get_file_tree (ID: call_12345)
    Input: {"dir_path": "."}
    Output Snippet: ... [DIR] google ...
  - read_definitions (ID: call_67890)
    Input: {"file_path": "google/adk/agents.py"}
    Output Snippet: Error: File not found ...
```

## Other Tools

*   `benchmark_viewer.py`: Streamlit app for visualizing results.
*   `list_models.py`: Lists available Vertex AI models.
*   `rebuild_images.py`: Rebuilds the docker images used for isolated execution.
