# Codebase Knowledge Generation Pipeline

This directory contains the tools necessary to generate the static artifacts needed by the **Codebase Knowledge MCP Server** to answer questions about the target codebase (ADK). 

Unlike typical MCP server implementations that run heavy language model inferences and code AST parsing at runtime, this system uses an **Offline Static Artifact Pipeline**. This isolates the heavy dependencies (like `google-genai` and `ast`) into the host developer environment, allowing the shipped MCP server to run instantly with an extremely lightweight footprint.

## Pipeline Architecture

The pipeline consists of two primary stages, which can be executed sequentially via the VS Code task: **`"Regenerate Codebase Knowledge (Targets & Embeddings)"`**.

### 1. Target Ranking (`target_ranker/`)
**Entrypoint:** `tools/knowledge/target_ranker/ranker.py`
- Acts as a "Cartographer" for the codebase.
- Uses AST to scan Python files, resolve inheritance (MRO), and extract signatures and docstrings.
- Calculates and assigns a "Rank" to each symbol based on its usage frequency in the source code / tests.
- **Output:** `benchmarks/generator/benchmark_generator/data/ranked_targets.yaml` and `ranked_targets.md`.

### 2. Semantic Indexing (`build_vector_index.py`)
**Entrypoint:** `tools/knowledge/build_vector_index.py`
- Consumes the `ranked_targets.yaml` file generated by the ranker.
- Uses the `google-genai` SDK (`text-embedding-004` or `gemini-embedding-001`) to generate high-dimensional vector embeddings for every FQN and docstring.
- Stores these vectors in a continuous, highly-optimized NumPy array.
- **Output:** `targets_vectors.npy` and `targets_meta.json` in the same directory as the `.yaml`.

---

## MCP Server Integration

When you start the Codebase Knowledge MCP Server (via `uvx`), it does **not** run these scripts:
1. It is built as a standalone package (`codebase-knowledge-mcp`) from `tools/adk_knowledge_ext`.
2. It lists only lightweight dependencies (`mcp`, `numpy`, `rank-bm25`, etc.).
3. During build or runtime, it loads the *pre-computed* `.npy` arrays and `.yaml` dictionaries directly into memory.
4. When a user queries the MCP server, it only executes a single embedding API call for the search query and performs a lightning-fast cosine similarity math operation against the loaded NumPy array.

This separation of concerns ensures your AI agent has instant, deep knowledge of the ADK without bloated dependencies.
