{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script to run benchmarks and analyze results.\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "import difflib\n",
    "import pandas as pd\n",
    "\n",
    "from benchmarks import benchmark_orchestrator\n",
    "from benchmarks.benchmark_candidates import CANDIDATE_GENERATORS\n",
    "from benchmarks.data_models import BenchmarkRunResult\n",
    "from benchmarks.logger import JsonTraceLogger\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727565c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes for colors\n",
    "class Bcolors:\n",
    "  HEADER = \"\\033[95m\"\n",
    "  OKBLUE = \"\\033[94m\"\n",
    "  OKCYAN = \"\\033[96m\"\n",
    "  OKGREEN = \"\\033[92m\"\n",
    "  WARNING = \"\\033[93m\"\n",
    "  FAIL = \"\\033[91m\"\n",
    "  ENDC = \"\\033[0m\"\n",
    "  BOLD = \"\\033[1m\"\n",
    "  UNDERLINE = \"\\033[4m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_comparison(logger: JsonTraceLogger) -> List[BenchmarkRunResult]:\n",
    "  \"\"\"Sets up and runs the benchmark comparison.\"\"\"\n",
    "  print(\"Configuring benchmark run...\")\n",
    "\n",
    "  benchmark_suites = [\n",
    "      \"benchmarks/benchmark_definitions/api_understanding/benchmark.yaml\",\n",
    "      \"benchmarks/benchmark_definitions/fix_errors/benchmark.yaml\",\n",
    "      \"benchmarks/benchmark_definitions/diagnose_setup_errors_mc/benchmark.yaml\",\n",
    "      \"benchmarks/benchmark_definitions/configure_adk_features_mc/benchmark.yaml\",\n",
    "      \"benchmarks/benchmark_definitions/predict_runtime_behavior_mc/benchmark.yaml\",\n",
    "  ]\n",
    "\n",
    "  answer_generators = CANDIDATE_GENERATORS\n",
    "\n",
    "  print(\"Executing benchmarks...\")\n",
    "  benchmark_results = await benchmark_orchestrator.run_benchmarks(\n",
    "      benchmark_suites=benchmark_suites,\n",
    "      answer_generators=answer_generators,\n",
    "      max_concurrency=20,\n",
    "      logger=logger,\n",
    "  )\n",
    "\n",
    "  return benchmark_results\n",
    "\n",
    "\n",
    "def extract_error_type(row) -> str:\n",
    "  \"\"\"Extracts error type from the result row.\"\"\"\n",
    "  if \"error_type\" in row and pd.notna(row[\"error_type\"]):\n",
    "    # If it's an Enum object (from pydantic validation), get its value\n",
    "    et = row[\"error_type\"]\n",
    "    if hasattr(et, \"value\"):\n",
    "      return et.value\n",
    "    return str(et)\n",
    "  return \"OtherError\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9541069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(\n",
    "    benchmark_run_results: List[BenchmarkRunResult],\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"Converts results to a DataFrame and adds derived columns.\"\"\"\n",
    "  raw_results_df = pd.DataFrame([r.model_dump() for r in benchmark_run_results])\n",
    "\n",
    "  if not raw_results_df.empty:\n",
    "    raw_results_df[\"suite\"] = raw_results_df[\"suite\"].apply(\n",
    "        lambda x: x.split(\"/\")[-2]\n",
    "    )\n",
    "    raw_results_df[\"final_error_type\"] = raw_results_df.apply(\n",
    "        extract_error_type, axis=1\n",
    "    )\n",
    "  return raw_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(raw_results_df: pd.DataFrame):\n",
    "  \"\"\"Prints a high-level summary of pass rates.\"\"\"\n",
    "  if raw_results_df.empty:\n",
    "    print(\"No results to summarize.\")\n",
    "    return\n",
    "\n",
    "  # 1. General Pass/Total Summary\n",
    "  summary_df = raw_results_df.groupby([\"answer_generator\", \"suite\"]).agg(\n",
    "      passed=(\"result\", \"sum\"),\n",
    "      total=(\"result\", \"count\"),\n",
    "  )\n",
    "  summary_df[\"pass_rate\"] = summary_df[\"passed\"] / summary_df[\"total\"]\n",
    "\n",
    "  print(f\"{Bcolors.HEADER}--- Benchmark Summary ---\\n{Bcolors.ENDC}\")\n",
    "  print(summary_df)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(raw_results_df: pd.DataFrame):\n",
    "  \"\"\"Prints performance and cost metrics.\"\"\"\n",
    "  if raw_results_df.empty:\n",
    "    return\n",
    "\n",
    "  # Extract usage metrics if available\n",
    "  df = raw_results_df.copy()\n",
    "\n",
    "  def get_meta(row, key):\n",
    "    if isinstance(row.get(\"usage_metadata\"), dict):\n",
    "      return row[\"usage_metadata\"].get(key, 0)\n",
    "    return 0\n",
    "\n",
    "  df[\"tokens\"] = df.apply(lambda r: get_meta(r, \"total_tokens\"), axis=1)\n",
    "  df[\"cost\"] = df.apply(lambda r: get_meta(r, \"cost\"), axis=1)\n",
    "\n",
    "  # 1. Per Answer Generator Breakdown\n",
    "  gen_metrics = df.groupby(\"answer_generator\").agg(\n",
    "      avg_latency=(\"latency\", \"mean\"),\n",
    "      avg_tokens=(\"tokens\", \"mean\"),\n",
    "      total_cost=(\"cost\", \"sum\"),\n",
    "      pass_rate=(\"result\", \"mean\"),\n",
    "      count=(\"result\", \"count\"),\n",
    "  )\n",
    "  gen_metrics = gen_metrics.rename(columns={\"avg_latency\": \"avg_latency (s)\"})\n",
    "\n",
    "  print(f\"{Bcolors.HEADER}--- Metrics by Answer Generator ---\\n{Bcolors.ENDC}\")\n",
    "  print(gen_metrics.round(4))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  # 2. Detailed Breakdown by Generator and Suite\n",
    "  detailed_metrics = df.groupby([\"answer_generator\", \"suite\"]).agg(\n",
    "      avg_latency=(\"latency\", \"mean\"),\n",
    "      avg_tokens=(\"tokens\", \"mean\"),\n",
    "      total_cost=(\"cost\", \"sum\"),\n",
    "      pass_rate=(\"result\", \"mean\"),\n",
    "      count=(\"result\", \"count\"),\n",
    "  )\n",
    "  detailed_metrics = detailed_metrics.rename(\n",
    "      columns={\"avg_latency\": \"avg_latency (s)\"}\n",
    "  )\n",
    "\n",
    "  print(f\"{Bcolors.HEADER}--- Metrics by Generator & Suite ---\\n{Bcolors.ENDC}\")\n",
    "  print(detailed_metrics.round(4))\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time_profiling(raw_results_df: pd.DataFrame):\n",
    "  \"\"\"Analyzes latency and execution time to identify bottlenecks.\"\"\"\n",
    "  if raw_results_df.empty:\n",
    "    return\n",
    "\n",
    "  print(f\"{Bcolors.HEADER}--- Time Profiling Analysis ---\\n{Bcolors.ENDC}\")\n",
    "\n",
    "  # 1. Latency Statistics\n",
    "  latency_stats = raw_results_df.groupby(\"answer_generator\")[\n",
    "      \"latency\"\n",
    "  ].describe(percentiles=[0.5, 0.75, 0.90, 0.95])[\n",
    "      [\"mean\", \"min\", \"50%\", \"90%\", \"95%\", \"max\"]\n",
    "  ]\n",
    "  print(\"Latency Statistics (seconds):\")\n",
    "  print(latency_stats.round(2))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  # 2. Slowest Benchmarks\n",
    "  print(\"Top 5 Slowest Benchmarks:\")\n",
    "  slowest = raw_results_df.nlargest(5, \"latency\")[\n",
    "      [\"answer_generator\", \"suite\", \"benchmark_name\", \"latency\", \"result\"]\n",
    "  ]\n",
    "  print(slowest.to_string(index=False))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  # 3. Complexity Analysis (Trace Logs)\n",
    "  # Count model calls and tool calls if trace logs exist\n",
    "  def count_trace_events(logs):\n",
    "    if not isinstance(logs, list):\n",
    "      return 0, 0\n",
    "    model_calls = sum(\n",
    "        1 for e in logs if e.get(\"type\") in [\"model_response\", \"model_call\"]\n",
    "    )\n",
    "    tool_calls = sum(\n",
    "        1 for e in logs if e.get(\"type\") in [\"tool_code\", \"tool_execution\"]\n",
    "    )\n",
    "    return model_calls, tool_calls\n",
    "\n",
    "  # Check if trace_logs column exists and has data\n",
    "  if \"trace_logs\" in raw_results_df.columns:\n",
    "    # Use apply with result_type='expand' to get two columns\n",
    "    counts = raw_results_df[\"trace_logs\"].apply(\n",
    "        lambda x: count_trace_events(x if isinstance(x, list) else [])\n",
    "    )\n",
    "    # Manually assign columns to avoid inconsistencies\n",
    "    raw_results_df[\"num_model_calls\"] = counts.apply(lambda x: x[0])\n",
    "    raw_results_df[\"num_tool_calls\"] = counts.apply(lambda x: x[1])\n",
    "\n",
    "    # Average calls per generator\n",
    "    call_stats = raw_results_df.groupby(\"answer_generator\")[\n",
    "        [\"num_model_calls\", \"num_tool_calls\", \"latency\"]\n",
    "    ].mean()\n",
    "    print(\"Average Complexity (Calls) vs Latency:\")\n",
    "    print(call_stats.round(2))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e74a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_detailed_breakdown(raw_results_df: pd.DataFrame):\n",
    "  \"\"\"Prints detailed error breakdown and Gemini CLI failures.\"\"\"\n",
    "  if raw_results_df.empty:\n",
    "    return\n",
    "\n",
    "  # Filter for failures only\n",
    "  failed_df = raw_results_df[raw_results_df[\"result\"] == 0]\n",
    "\n",
    "  if not failed_df.empty:\n",
    "    # Calculate counts per error type\n",
    "    error_counts = (\n",
    "        failed_df.groupby([\"answer_generator\", \"suite\", \"final_error_type\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "\n",
    "    # Merge with total counts to calculate ratios relative to total runs\n",
    "    # First, get total counts per generator/suite group\n",
    "    total_counts = (\n",
    "        raw_results_df.groupby([\"answer_generator\", \"suite\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"total_runs\")\n",
    "    )\n",
    "\n",
    "    # Merge error counts with totals\n",
    "    error_summary = pd.merge(\n",
    "        error_counts, total_counts, on=[\"answer_generator\", \"suite\"]\n",
    "    )\n",
    "\n",
    "    # Calculate failure rate for each specific error type\n",
    "    error_summary[\"failure_ratio\"] = (\n",
    "        error_summary[\"count\"] / error_summary[\"total_runs\"]\n",
    "    )\n",
    "\n",
    "    print(f\"{Bcolors.HEADER}--- Detailed Error Breakdown ---\\n{Bcolors.ENDC}\")\n",
    "    # Sort for better readability\n",
    "    error_summary = error_summary.sort_values(\n",
    "        [\"answer_generator\", \"suite\", \"count\"], ascending=[True, True, False]\n",
    "    )\n",
    "    print(error_summary.to_string(index=False))\n",
    "\n",
    "    # --- DETAILED DEBUG FOR GEMINI CLI FAILURES ---\n",
    "    print(\n",
    "        f\"\\n{Bcolors.FAIL}--- DETAILED GEMINI CLI FAILURES ---\\n{Bcolors.ENDC}\"\n",
    "    )\n",
    "    cli_failures = failed_df[\n",
    "        failed_df[\"answer_generator\"].str.contains(\"GeminiCliAnswerGenerator\")\n",
    "    ]\n",
    "    if not cli_failures.empty:\n",
    "      # Print just the first 3 failures to avoid overwhelming output\n",
    "      for _, failure_row in cli_failures.head(3).iterrows():\n",
    "        print(\n",
    "            f\"\\nBenchmark: {failure_row['benchmark_name']} (Suite:\"\n",
    "            f\" {failure_row['suite']})\"\n",
    "        )\n",
    "        print(f\"Error Type: {failure_row['final_error_type']}\")\n",
    "        print(f\"Full Validation Error:\\n{failure_row['validation_error']}\")\n",
    "        print(\"-\" * 60)\n",
    "    else:\n",
    "      print(\"No Gemini CLI failures found in this run.\")\n",
    "    # -----------------------------------------------\n",
    "  else:\n",
    "    print(f\"{Bcolors.OKGREEN}No failures detected!{Bcolors.ENDC}\")\n",
    "\n",
    "\n",
    "def strip_ansi(text: str) -> str:\n",
    "  \"\"\"Strips ANSI escape codes from the text.\"\"\"\n",
    "  ansi_escape = re.compile(r\"\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])\")\n",
    "  return ansi_escape.sub(\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c446dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_detailed_reports(raw_results_df: pd.DataFrame, output_dir: Path):\n",
    "  \"\"\"Generates detailed Markdown reports for each answer generator.\"\"\"\n",
    "  if raw_results_df.empty:\n",
    "    return\n",
    "\n",
    "  print(f\"\\n{Bcolors.HEADER}--- Generating Detailed Reports ---{Bcolors.ENDC}\")\n",
    "  print(f\"Output Directory: {output_dir.absolute()}\")\n",
    "\n",
    "  for generator, group in raw_results_df.groupby(\"answer_generator\"):\n",
    "    report_lines = [f\"# Benchmark Report: {generator}\", \"\"]\n",
    "\n",
    "    # Summary\n",
    "    total = len(group)\n",
    "    passed = len(group[group[\"result\"] == 1])\n",
    "    pass_rate = (passed / total) * 100 if total > 0 else 0\n",
    "\n",
    "    report_lines.extend([\n",
    "        \"## Summary\",\n",
    "        f\"- **Total Cases:** {total}\",\n",
    "        f\"- **Passed:** {passed}\",\n",
    "        f\"- **Pass Rate:** {pass_rate:.2f}%\",\n",
    "        \"\",\n",
    "        \"## Details\",\n",
    "        \"\",\n",
    "    ])\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "      status_icon = \"✅\" if row[\"result\"] == 1 else \"❌\"\n",
    "      report_lines.extend([\n",
    "          f\"### {status_icon} {row['benchmark_name']}\",\n",
    "          f\"- **Suite:** {row['suite']}\",\n",
    "          f\"- **Status:** {row['status']}\",\n",
    "          f\"- **Error Type:** {row['final_error_type']}\",\n",
    "          \"\",\n",
    "      ])\n",
    "\n",
    "      if row[\"validation_error\"]:\n",
    "        cleaned_error = strip_ansi(str(row[\"validation_error\"]))\n",
    "        report_lines.extend(\n",
    "            [\"**Validation Error:**\", \"```\", cleaned_error, \"```\", \"\"]\n",
    "        )\n",
    "\n",
    "      if row[\"rationale\"]:\n",
    "        cleaned_rationale = strip_ansi(str(row[\"rationale\"]))\n",
    "        report_lines.extend([\"**Rationale:**\", cleaned_rationale, \"\"])\n",
    "\n",
    "        report_lines.extend([\n",
    "            \"**Generated Answer:**\",\n",
    "            \"```python\",\n",
    "            str(row[\"answer\"]),\n",
    "            \"```\",\n",
    "        ])\n",
    "\n",
    "        # Add diff for fix_errors benchmarks\n",
    "        if row[\"benchmark_type\"] == \"fix_error\":\n",
    "          fixed_content = row.get(\"ground_truth\", \"\") or \"\"\n",
    "\n",
    "          generated_content = str(row[\"answer\"])\n",
    "          diff = difflib.unified_diff(\n",
    "              [l.rstrip() for l in fixed_content.splitlines(keepends=True)],\n",
    "              [l.rstrip() for l in generated_content.splitlines(keepends=True)],\n",
    "              fromfile=\"expected/fixed.py\",\n",
    "              tofile=\"generated/answer.py\",\n",
    "          )\n",
    "\n",
    "          diff_text = \"\\n\".join(diff)\n",
    "\n",
    "          if diff_text:\n",
    "            report_lines.extend([\n",
    "                \"**Diff (Expected vs. Generated):**\",\n",
    "                \"```diff\",\n",
    "                diff_text,\n",
    "                \"```\",\n",
    "            ])\n",
    "          else:\n",
    "            report_lines.extend([\n",
    "                \"**Diff (Expected vs. Generated):**\",\n",
    "                \"```\",\n",
    "                \"No differences.\",\n",
    "                \"```\",\n",
    "            ])\n",
    "\n",
    "        report_lines.extend([\"---\", \"\"])\n",
    "\n",
    "    # Sanitize filename\n",
    "    safe_name = \"\".join(\n",
    "        c if c.isalnum() or c in \"._- \" else \"_\" for c in generator\n",
    "    )\n",
    "    file_path = output_dir / f\"{safe_name}_report.md\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "      f.write(\"\\n\".join(report_lines))\n",
    "      print(f\"  - Report saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup unified output directory\n",
    "current_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_output_dir = Path(\"benchmark_runs\") / current_timestamp\n",
    "run_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize logger\n",
    "logger = JsonTraceLogger(output_dir=str(run_output_dir), filename=\"trace.jsonl\")\n",
    "\n",
    "# Execute the benchmarks\n",
    "benchmark_run_results = await run_comparison(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_results_df = process_results(benchmark_run_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a62408",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not raw_results_df.empty:\n",
    "  print_summary(raw_results_df)\n",
    "  print_metrics(raw_results_df)\n",
    "  print_time_profiling(raw_results_df)\n",
    "  print_detailed_breakdown(raw_results_df)\n",
    "  generate_detailed_reports(raw_results_df, output_dir=run_output_dir)\n",
    "else:\n",
    "  print(\"No results returned.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
