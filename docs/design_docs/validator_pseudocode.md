# Validator Pseudocode: Statistical Impact Scoring via Context Injection

## Overview
We quantify the relevance of specific contexts (FQNs) by measuring their **impact** on the success rate of solving a benchmark query. The validator injects randomized subsets of context directly into the prompt and measures the causal lift in success rate.

## 1. Candidate Selection (Pooling)

Before running trials, we must establish a diverse pool of candidates to evaluate.

```python
async def _get_initial_candidates(self, case: RetrievalCase) -> List[RetrievalContext]:
    """
    Constructs the universe of candidates for the Monte Carlo simulation.
    """
    candidates = {}
    
    # A. Seeded "Gold" Candidates (from Benchmark Mining)
    # Generated by `tools/extract_retrieval_data.py`:
    # - API Understanding: Extracted from `fully_qualified_class_name` field in benchmark definition.
    # - Fix Errors: Inferred by parsing imports from the ground truth solution file (`fixed.py`).
    for ctx in case.positive_ctxs:
        candidates[ctx.fqn] = ctx
        
    # B. Hard Negatives / Missed Positives (Vector Search)
    # We use a strong Retriever to find what the model *might* retrieve.
    retrieved = await self.retriever.search(case.query, top_k=10)
    for t in retrieved:
        if t.id not in candidates:
            candidates[t.id] = RetrievalContext(
                fqn=t.id, text=t.docstring, type="retrieved"
            )
            
    # C. Random Negatives (Noise)
    # Control group to measure baseline performance with noise.
    # Generated by `tools/extract_retrieval_data.py` via random sampling.
    for ctx in case.negative_ctxs:
        if ctx.fqn not in candidates:
            candidates[ctx.fqn] = ctx
            
    return list(candidates.values())
```

## 2. Monte Carlo Validation Loop

```python
class DataValidator:
    
    async def validate_case(self, case: RetrievalCase):
        """
        Quantifies the impact of each context via Monte Carlo trials.
        """
        
        # 1. Candidate Pool
        candidates = await self._get_initial_candidates(case)
        fqn_map = {c.fqn: c for c in candidates}
        
        # Statistics Containers
        # trials_in[fqn]: # of trials where FQN was PRESENT
        # success_in[fqn]: # of SUCCESSFUL trials where FQN was PRESENT
        stats = {
            fqn: {
                'success_in': 0, 'trials_in': 0, 
                'success_out': 0, 'trials_out': 0
            } 
            for fqn in fqn_map
        }
        
        # 2. Monte Carlo Loop
        for _ in range(N_TRIALS):
            # a. Sample a subset (Bernoulli p=0.5)
            # This forms the "Available Knowledge Kernel" for this trial.
            subset_fqns = sample(candidates, p=0.5)
            subset_ctxs = [fqn_map[f] for f in subset_fqns]
            
            # b. Construct Prompt with Injected Context
            # Direct injection: The model sees ONLY these documents.
            combined_context = "\n".join([
                f"--- Document: {c.fqn} ---\n{c.text}\n" 
                for c in subset_ctxs
            ])
            
            # c. Attempt Task (Single Shot)
            # The model attempts to solve the task using ONLY the injected context.
            answer = await self.generate_answer(case.query, combined_context)
            
            # d. Validate Answer
            # Use the official benchmark runner to determine correctness (Pass/Fail).
            is_correct = self.benchmark_runner.validate(case, answer)
            
            # e. Update Stats (Causal Attribution)
            for fqn in fqn_map:
                if fqn in subset_fqns:
                    stats[fqn]['trials_in'] += 1
                    if is_correct: stats[fqn]['success_in'] += 1
                else:
                    stats[fqn]['trials_out'] += 1
                    if is_correct: stats[fqn]['success_out'] += 1

        # 3. Calculate Impact Scores (Delta P)
        # Impact = P(Success | Context Present) - P(Success | Context Absent)
        final_contexts = []
        for fqn in stats:
            p_in = stats[fqn]['success_in'] / stats[fqn]['trials_in'] if stats[fqn]['trials_in'] > 0 else 0
            p_out = stats[fqn]['success_out'] / stats[fqn]['trials_out'] if stats[fqn]['trials_out'] > 0 else 0
            
            impact_score = p_in - p_out
            
            ctx = fqn_map[fqn]
            ctx.metadata['impact_score'] = impact_score
            ctx.metadata['p_in'] = p_in
            ctx.metadata['p_out'] = p_out
            final_contexts.append(ctx)

        # The final dataset contains the raw scalar scores.
        # We replace the original lists with this unified, scored list.
        case.candidates = final_contexts 
        return case
```